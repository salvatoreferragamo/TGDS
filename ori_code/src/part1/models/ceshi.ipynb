{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找寻连续或者独立的序列：\n",
    "topic_utter_ids = [8,12,13,14,15,48,56]\n",
    "start_ids = []\n",
    "end_ids = []\n",
    "# 一个队列结构的list用于输出连续性utterance id\n",
    "queue = []\n",
    "for i, idx in enumerate(topic_utter_ids):\n",
    "    # 初始化第一个元素：\n",
    "    if i == 0: \n",
    "        queue.append(idx)\n",
    "    else:\n",
    "        if queue[-1]+1 == idx:\n",
    "            queue.append(idx)\n",
    "        else:\n",
    "            start_ids.append(queue[0])\n",
    "            end_ids.append(queue[-1])\n",
    "            queue = []\n",
    "            queue.append(idx)\n",
    "start_ids.append(queue[0])\n",
    "end_ids.append(queue[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 12, 48, 56], [8, 15, 48, 56])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_ids, end_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-116f3d0e30e1>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-116f3d0e30e1>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    cur_pair.extend([s+target_shift,e+target_shift for (s,e) in zip(start_ids,end_ids)])\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cur_pair = []\n",
    "target_shift = 20\n",
    "for s,e in zip(start_ids,end_ids):\n",
    "    cur_pair.extend([s+target_shift,e+target_shift])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 28, 32, 35, 68, 68, 76, 76]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 我  真  是   服  了 '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../pretrained/bert_base_chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0413)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "model = BertModel.from_pretrained('../../pretrained/bert_base_chinese')\n",
    "num_tokens, _ = model.embeddings.word_embeddings.weight.shape\n",
    "model.resize_token_embeddings(21190)\n",
    "# model.embeddings.word_embeddings.weight.data[21189]\n",
    "model.embeddings.word_embeddings.weight.data[21188][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0060)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.word_embeddings.weight.data[21189][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Tokenization classes.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "from transformers import cached_path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# PRETRAINED_VOCAB_ARCHIVE_MAP = {\n",
    "#     'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n",
    "#     'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n",
    "#     'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n",
    "#     'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n",
    "#     'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n",
    "#     'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n",
    "#     'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n",
    "# }\n",
    "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n",
    "    'bert-base-uncased': 512,\n",
    "    'bert-large-uncased': 512,\n",
    "    'bert-base-cased': 512,\n",
    "    'bert-large-cased': 512,\n",
    "    'bert-base-multilingual-uncased': 512,\n",
    "    'bert-base-multilingual-cased': 512,\n",
    "    'bert-base-chinese': 512,\n",
    "}\n",
    "VOCAB_NAME = 'vocab.txt'\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab  # collections.OrderedDict\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens  # list\n",
    "\n",
    "\n",
    "class BertTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenization: punctuation splitting + wordpiece\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True, mapping=None, max_len=None,\n",
    "                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[unused98]\", \"[unused99]\", \"[unused1]\", \"[unused2]\", \"[unused3]\", \"[unused4]\", \"[unused5]\", \"[unused6]\")):\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n",
    "        self.do_lower_case = do_lower_case\n",
    "        # 将Tag 内容增加至 never split中去：\n",
    "        tokens_to_add = sorted(list(mapping.values()), key=lambda x: len(x), reverse=True)\n",
    "        sorted_add_tokens = sorted(list(tokens_to_add), key=lambda x: len(x), reverse=True)\n",
    "        self.never_split = never_split + tuple(sorted_add_tokens)\n",
    "        # 对Bert字典进行添加\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        orig_vocab_len = len(self.vocab)\n",
    "        for idx, add_token in enumerate(sorted_add_tokens):\n",
    "            self.vocab[add_token] = idx+orig_vocab_len\n",
    "        \n",
    "        # 对新的vocab生成字典用于映射\n",
    "        self.ids_to_tokens = collections.OrderedDict(\n",
    "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
    "\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
    "                                              never_split=self.never_split)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "        self.max_len = max_len if max_len is not None else int(1e12)\n",
    "\n",
    "    def tokenize(self, text, use_bert_basic_tokenizer=False):\n",
    "        split_tokens = []\n",
    "        # 此部分工作的意义在于： \n",
    "        if(use_bert_basic_tokenizer):\n",
    "            # 经过jieba分好词的结果组成的text\n",
    "            pretokens = self.basic_tokenizer.tokenize(text)\n",
    "        else:\n",
    "            # 经过jieba分好词的结果\n",
    "            pretokens = list(enumerate(text.split()))\n",
    "\n",
    "        for i,token in pretokens:\n",
    "            # if(self.do_lower_case):\n",
    "            #     token = token.lower()\n",
    "            subtokens = self.wordpiece_tokenizer.tokenize(token)\n",
    "            for sub_token in subtokens:\n",
    "                # 将分词后的bpe之后的结果进行整合\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab[token])\n",
    "        # if len(ids) > self.max_len:\n",
    "        #     raise ValueError(\n",
    "        #         \"Token indices sequence length is longer than the specified maximum \"\n",
    "        #         \" sequence length for this BERT model ({} > {}). Running this\"\n",
    "        #         \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\n",
    "        #     )\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.ids_to_tokens[i])\n",
    "        return tokens\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiate a PreTrainedBertModel from a pre-trained model file.\n",
    "        Download and cache the pre-trained model file if needed.\n",
    "        \"\"\"\n",
    "        # if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n",
    "        #     vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
    "        # else:\n",
    "        vocab_file = pretrained_model_name_or_path\n",
    "        if os.path.isdir(vocab_file):\n",
    "            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n",
    "        # redirect to the cache, if necessary\n",
    "        # try:\n",
    "        #     resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n",
    "        # except EnvironmentError:\n",
    "        #     logger.error(\n",
    "        #         \"Model name '{}' was not found in model name list ({}). \"\n",
    "        #         \"We assumed '{}' was a path or url but couldn't find any file \"\n",
    "        #         \"associated to this path or url.\".format(\n",
    "        #             pretrained_model_name_or_path,\n",
    "        #             ', '.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n",
    "        #             vocab_file))\n",
    "        #     return None\n",
    "        # if resolved_vocab_file == vocab_file:\n",
    "        #     logger.info(\"loading vocabulary file {}\".format(vocab_file))\n",
    "        # else:\n",
    "        #     logger.info(\"loading vocabulary file {} from cache at {}\".format(\n",
    "        #         vocab_file, resolved_vocab_file))\n",
    "        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n",
    "            # if we're using a pretrained model, ensure the tokenizer wont index sequences longer\n",
    "            # than the number of positional embeddings\n",
    "            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n",
    "            kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n",
    "        # Instantiate tokenizer.\n",
    "        tokenizer = cls(vocab_file, *inputs, **kwargs)\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 do_lower_case=True,\n",
    "                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n",
    "        \"\"\"Constructs a BasicTokenizer.\n",
    "\n",
    "        Args:\n",
    "          do_lower_case: Whether to lower case the input.\n",
    "        \"\"\"\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = never_split\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "        text = self._clean_text(text)\n",
    "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "        # models. This is also applied to the English models now, but it doesn't\n",
    "        # matter since the English models were not trained on any Chinese data\n",
    "        # and generally don't have any Chinese data in them (there are Chinese\n",
    "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "        # words in the English Wikipedia.).\n",
    "        text = self._tokenize_chinese_chars(text)\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for i,token in enumerate(orig_tokens):\n",
    "            if self.do_lower_case and token not in self.never_split:\n",
    "                token = token.lower()\n",
    "                token = self._run_strip_accents(token)\n",
    "            # split_tokens.append(token)\n",
    "            split_tokens.extend([(i,t) for t in self._run_split_on_punc(token)])\n",
    "\n",
    "        # output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return split_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        if text in self.never_split:\n",
    "            return [text]\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "        #\n",
    "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "        # space-separated words, so they are not treated specially and handled\n",
    "        # like the all of the other languages.\n",
    "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  ##\n",
    "                (cp >= 0x3400 and cp <= 0x4DBF) or  ##\n",
    "                (cp >= 0x20000 and cp <= 0x2A6DF) or  ##\n",
    "                (cp >= 0x2A700 and cp <= 0x2B73F) or  ##\n",
    "                (cp >= 0x2B740 and cp <= 0x2B81F) or  ##\n",
    "                (cp >= 0x2B820 and cp <= 0x2CEAF) or  \n",
    "                (cp >= 0xF900 and cp <= 0xFAFF) or  ##\n",
    "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  ##\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "\n",
    "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "        using the given vocabulary.\n",
    "\n",
    "        For example:\n",
    "          input = \"unaffable\"\n",
    "          output = [\"un\", \"##aff\", \"##able\"]\n",
    "\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer`.\n",
    "\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n",
    "\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mapping = {'我操了':'<<我操了>>'}\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('../../pretrained/bert_base_chinese', do_lower_case=True, mapping=tag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21128]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '真', '是', '<<我操了>>', '[unused99]']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer.tokenize('我 真 是 <<我操了>> [unused99]')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2769, 4696, 3221, 21128, 99]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_utter_ids = [1,3,6,9,12]\n",
    "queue = []\n",
    "start_ids = []\n",
    "end_ids = []\n",
    "for i, idx in enumerate(topic_utter_ids):\n",
    "    # 初始化第一个元素：\n",
    "    if i == 0: \n",
    "        queue.append(idx)\n",
    "    else:\n",
    "        if queue[-1]+1 == idx:\n",
    "            queue.append(idx)\n",
    "        else:\n",
    "            start_ids.append(queue[0])\n",
    "            end_ids.append(queue[-1])\n",
    "            queue = []\n",
    "            queue.append(idx)\n",
    "start_ids.append(queue[0])\n",
    "end_ids.append(queue[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 3, 6, 9, 12], [1, 3, 6, 9, 12])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_ids,end_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [1,2,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(a) | set(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/CSDS/dict/label2tag.json','r',encoding = 'utf-8') as js_object:\n",
    "    a = json.load(js_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'联系售后': '<<联系售后>>',\n",
       " '商品介绍': '<<商品介绍>>',\n",
       " '商品检索': '<<商品检索>>',\n",
       " '返回方式': '<<返回方式>>',\n",
       " '自营与第三方区别': '<<自营与第三方区别>>',\n",
       " '话费充值': '<<话费充值>>',\n",
       " '售后维修点查询': '<<售后维修点查询>>',\n",
       " '支付密码': '<<支付密码>>',\n",
       " '查看发票': '<<查看发票>>',\n",
       " '联系客服': '<<联系客服>>',\n",
       " '联系商家': '<<联系商家>>',\n",
       " '京豆积分扣除': '<<京豆积分扣除>>',\n",
       " '填写发票信息': '<<填写发票信息>>',\n",
       " '取消订单受理时间': '<<取消订单受理时间>>',\n",
       " '服务单修改': '<<服务单修改>>',\n",
       " '支付到账时间': '<<支付到账时间>>',\n",
       " '是否全国联保': '<<是否全国联保>>',\n",
       " '是否送货上门': '<<是否送货上门>>',\n",
       " '安装收费': '<<安装收费>>',\n",
       " '代客户充值': '<<代客户充值>>',\n",
       " '为什么显示无货': '<<为什么显示无货>>',\n",
       " '订单回收站': '<<订单回收站>>',\n",
       " '评价晒单异常': '<<评价晒单异常>>',\n",
       " '预约抢购': '<<预约抢购>>',\n",
       " '下单后无记录': '<<下单后无记录>>',\n",
       " '预售订单': '<<预售订单>>',\n",
       " '其他': '<<其他>>',\n",
       " '小金库转入转出': '<<小金库转入转出>>',\n",
       " '查询个人资料': '<<查询个人资料>>',\n",
       " '返修退换货审核不通过': '<<返修退换货审核不通过>>',\n",
       " '京豆积分比值': '<<京豆积分比值>>',\n",
       " '白条分期手续费': '<<白条分期手续费>>',\n",
       " '京豆积分解释': '<<京豆积分解释>>',\n",
       " '京东钱包绑定': '<<京东钱包绑定>>',\n",
       " '恢复订单': '<<恢复订单>>',\n",
       " '如何评价晒单': '<<如何评价晒单>>',\n",
       " '白条还款方式': '<<白条还款方式>>',\n",
       " '京豆积分查看': '<<京豆积分查看>>',\n",
       " '近期活动咨询': '<<近期活动咨询>>',\n",
       " '有什么颜色': '<<有什么颜色>>',\n",
       " '申请退款': '<<申请退款>>',\n",
       " '账户注册': '<<账户注册>>',\n",
       " '订单合并': '<<订单合并>>',\n",
       " '充值未到账充值到账时间': '<<充值未到账充值到账时间>>',\n",
       " '检测单咨询': '<<检测单咨询>>',\n",
       " '企业用户': '<<企业用户>>',\n",
       " '京豆积分获得方式': '<<京豆积分获得方式>>',\n",
       " '催促处理纠纷单': '<<催促处理纠纷单>>',\n",
       " '促销形式': '<<促销形式>>',\n",
       " '优惠券有效期': '<<优惠券有效期>>',\n",
       " '白条支持的订单类型': '<<白条支持的订单类型>>',\n",
       " '退款手续费': '<<退款手续费>>',\n",
       " '在线支付': '<<在线支付>>',\n",
       " '部分商品退款': '<<部分商品退款>>',\n",
       " '商家入驻联系方式': '<<商家入驻联系方式>>',\n",
       " '分期付款': '<<分期付款>>',\n",
       " '无法购买提交': '<<无法购买提交>>',\n",
       " '拆分订单': '<<拆分订单>>',\n",
       " '补货时间': '<<补货时间>>',\n",
       " '回收时间': '<<回收时间>>',\n",
       " '发票退换修改': '<<发票退换修改>>',\n",
       " '什么时间出库': '<<什么时间出库>>',\n",
       " '怎么确认收货': '<<怎么确认收货>>',\n",
       " '物流全程跟踪': '<<物流全程跟踪>>',\n",
       " '礼品卡退回': '<<礼品卡退回>>',\n",
       " '电子发票': '<<电子发票>>',\n",
       " '如何取消订单': '<<如何取消订单>>',\n",
       " '物流信息不正确': '<<物流信息不正确>>',\n",
       " '白条使用流程': '<<白条使用流程>>',\n",
       " '配件推荐': '<<配件推荐>>',\n",
       " '能否配送': '<<能否配送>>',\n",
       " '购物清单': '<<购物清单>>',\n",
       " '礼品卡查看及有效期': '<<礼品卡查看及有效期>>',\n",
       " '找回密码': '<<找回密码>>',\n",
       " '微信支付': '<<微信支付>>',\n",
       " '订单历史记录查询': '<<订单历史记录查询>>',\n",
       " '会员俱乐部': '<<会员俱乐部>>',\n",
       " '补发票': '<<补发票>>',\n",
       " '公司转账': '<<公司转账>>',\n",
       " '联系配送': '<<联系配送>>',\n",
       " '保修期保质期': '<<保修期保质期>>',\n",
       " '账户安全': '<<账户安全>>',\n",
       " '取消订单白条处理': '<<取消订单白条处理>>',\n",
       " '订单签收异常': '<<订单签收异常>>',\n",
       " '商品比较': '<<商品比较>>',\n",
       " '充值到账异常': '<<充值到账异常>>',\n",
       " '能否自提': '<<能否自提>>',\n",
       " '拒收': '<<拒收>>',\n",
       " '发货检查': '<<发货检查>>',\n",
       " '是否退定金': '<<是否退定金>>',\n",
       " '礼品卡绑定': '<<礼品卡绑定>>',\n",
       " '我的小金库': '<<我的小金库>>',\n",
       " '退款到哪儿': '<<退款到哪儿>>',\n",
       " '下单备注': '<<下单备注>>',\n",
       " '京东ID': '<<京东ID>>',\n",
       " '充值号码错误': '<<充值号码错误>>',\n",
       " '退款异常': '<<退款异常>>',\n",
       " '手机邮件相关问题': '<<手机邮件相关问题>>',\n",
       " '代收': '<<代收>>',\n",
       " '礼品卡获得': '<<礼品卡获得>>',\n",
       " '京豆积分退回': '<<京豆积分退回>>',\n",
       " '微信订单查询': '<<微信订单查询>>',\n",
       " '京东特色配送': '<<京东特色配送>>',\n",
       " 'PLUS会员': '<<PLUS会员>>',\n",
       " '登录问题': '<<登录问题>>',\n",
       " '优惠券查看': '<<优惠券查看>>',\n",
       " '超期未还款': '<<超期未还款>>',\n",
       " '快递单号不正确': '<<快递单号不正确>>',\n",
       " '价保条件': '<<价保条件>>',\n",
       " '无法使用优惠券': '<<无法使用优惠券>>',\n",
       " '删除修改评价晒单': '<<删除修改评价晒单>>',\n",
       " '免密支付': '<<免密支付>>',\n",
       " '解绑和绑定手机qq': '<<解绑和绑定手机qq>>',\n",
       " '出库地址': '<<出库地址>>',\n",
       " '正品保障': '<<正品保障>>',\n",
       " '支付异常': '<<支付异常>>',\n",
       " '属性咨询': '<<属性咨询>>',\n",
       " '提前配送': '<<提前配送>>',\n",
       " '白条额度': '<<白条额度>>',\n",
       " '查询取消是否成功': '<<查询取消是否成功>>',\n",
       " '余额提现': '<<余额提现>>',\n",
       " '发错货': '<<发错货>>',\n",
       " '填写返件运单号': '<<填写返件运单号>>',\n",
       " '订单状态解释': '<<订单状态解释>>',\n",
       " '解锁锁定': '<<解锁锁定>>',\n",
       " '回收订单取消': '<<回收订单取消>>',\n",
       " '无法加入购物车': '<<无法加入购物车>>',\n",
       " '手机回收流程': '<<手机回收流程>>',\n",
       " '商家入驻类目': '<<商家入驻类目>>',\n",
       " '评价晒单送积分京豆': '<<评价晒单送积分京豆>>',\n",
       " '商家入驻费用': '<<商家入驻费用>>',\n",
       " '修改账户信息': '<<修改账户信息>>',\n",
       " '余额退款': '<<余额退款>>',\n",
       " '少商品与少配件': '<<少商品与少配件>>',\n",
       " '纠纷单申请': '<<纠纷单申请>>',\n",
       " '返回地址': '<<返回地址>>',\n",
       " '账户注销': '<<账户注销>>',\n",
       " '评价晒单返券和赠品': '<<评价晒单返券和赠品>>',\n",
       " '返修退换货处理周期': '<<返修退换货处理周期>>',\n",
       " '延保服务': '<<延保服务>>',\n",
       " '赠品领取更换': '<<赠品领取更换>>',\n",
       " '支付方式': '<<支付方式>>',\n",
       " '包装清单': '<<包装清单>>',\n",
       " '服务单查询': '<<服务单查询>>',\n",
       " '下单地址填写': '<<下单地址填写>>',\n",
       " '纠纷单处理时效': '<<纠纷单处理时效>>',\n",
       " '普通会员': '<<普通会员>>',\n",
       " '使用咨询': '<<使用咨询>>',\n",
       " '联系客户': '<<联系客户>>',\n",
       " '库存状态': '<<库存状态>>',\n",
       " '补发': '<<补发>>',\n",
       " '预约配送时间': '<<预约配送时间>>',\n",
       " '回收电话': '<<回收电话>>',\n",
       " '节能补贴': '<<节能补贴>>',\n",
       " '回收款项': '<<回收款项>>',\n",
       " '价保申请流程': '<<价保申请流程>>',\n",
       " '订单无故取消': '<<订单无故取消>>',\n",
       " '是否回收': '<<是否回收>>',\n",
       " '优惠券获得方式': '<<优惠券获得方式>>',\n",
       " '消费记录': '<<消费记录>>',\n",
       " '配送方式': '<<配送方式>>',\n",
       " '是什么颜色': '<<是什么颜色>>',\n",
       " '价保记录查询': '<<价保记录查询>>',\n",
       " '修改订单': '<<修改订单>>',\n",
       " '开箱验货': '<<开箱验货>>',\n",
       " '忘记账户名': '<<忘记账户名>>',\n",
       " '纠纷单申请京东介入': '<<纠纷单申请京东介入>>',\n",
       " '京豆积分有效期': '<<京豆积分有效期>>',\n",
       " '增票相关': '<<增票相关>>',\n",
       " '返修退换货发票': '<<返修退换货发票>>',\n",
       " '白条开通': '<<白条开通>>',\n",
       " '纠纷单解释': '<<纠纷单解释>>',\n",
       " '关闭服务单': '<<关闭服务单>>',\n",
       " '能否优惠': '<<能否优惠>>',\n",
       " '购物流程': '<<购物流程>>',\n",
       " '家电安装': '<<家电安装>>',\n",
       " '无法申请价保': '<<无法申请价保>>',\n",
       " '商品价格咨询': '<<商品价格咨询>>',\n",
       " '货到付款': '<<货到付款>>',\n",
       " '在哪里查询退款': '<<在哪里查询退款>>',\n",
       " '白条注销': '<<白条注销>>',\n",
       " '查看评价晒单': '<<查看评价晒单>>',\n",
       " '优惠券退回': '<<优惠券退回>>',\n",
       " '京豆积分使用': '<<京豆积分使用>>',\n",
       " '保修返修及退换货政策': '<<保修返修及退换货政策>>',\n",
       " '商家入驻条件': '<<商家入驻条件>>',\n",
       " '余额查询': '<<余额查询>>',\n",
       " '物流损': '<<物流损>>',\n",
       " '生产日期': '<<生产日期>>',\n",
       " '礼品卡使用': '<<礼品卡使用>>',\n",
       " '售后图片上传': '<<售后图片上传>>',\n",
       " '是否提供发票': '<<是否提供发票>>',\n",
       " '套装购买': '<<套装购买>>',\n",
       " '解绑和绑定微信': '<<解绑和绑定微信>>',\n",
       " '充值号码咨询': '<<充值号码咨询>>',\n",
       " '退款通知': '<<退款通知>>',\n",
       " '取消退款': '<<取消退款>>',\n",
       " '返修退换货拆包装': '<<返修退换货拆包装>>',\n",
       " '自提时间': '<<自提时间>>',\n",
       " '外包装': '<<外包装>>',\n",
       " '配送周期': '<<配送周期>>',\n",
       " '实名认证与解除': '<<实名认证与解除>>',\n",
       " '售后运费': '<<售后运费>>',\n",
       " '配送工作时间': '<<配送工作时间>>',\n",
       " '运费险咨询': '<<运费险咨询>>',\n",
       " '优惠券使用': '<<优惠券使用>>',\n",
       " '查看纠纷单': '<<查看纠纷单>>',\n",
       " '正常退款周期': '<<正常退款周期>>',\n",
       " '审核时效': '<<审核时效>>',\n",
       " '售前运费多少': '<<售前运费多少>>'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "x = torch.tensor([[[1]]])\n",
    "assert x.size() == (1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [2, 2, 2, 2],\n",
       "         [3, 3, 3, 3],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[3, 4, 4, 4],\n",
       "         [1, 2, 2, 2],\n",
       "         [2, 3, 3, 3],\n",
       "         [0, 1, 1, 1],\n",
       "         [1, 2, 2, 2],\n",
       "         [0, 1, 1, 1]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sents_vec = torch.tensor([[[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]],[[0,1,1,1],[1,2,2,2],[2,3,3,3],[3,4,4,4]]])\n",
    "src_tokens_index = torch.tensor([[0,1,2,0,0,0],[3,1,2,0,1,0]])\n",
    "all = []\n",
    "for i in range(sents_vec.size(0)):\n",
    "    all.append(torch.index_select(sents_vec[i], 0, src_tokens_index[i]))\n",
    "# b, tgt_len, h\n",
    "torch.stack(all)\n",
    "# sents_vec.gather(index=src_tokens_index.unsqueeze(-1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_tgt = torch.tensor([[1,2,3,4,56]])\n",
    "topic_tgt.masked_fill(topic_tgt.ge(10), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([True,False]).unsqueeze(-1)\n",
    "# a.unsqueeze(-1)\n",
    "b=torch.tensor([[1,2],[1,3]])\n",
    "\n",
    "c=torch.tensor([[0,0],[0,0]])\n",
    "# b=torch.tensor([[1,2],[3,4]])\n",
    "# c=torch.tensor([[0,0],[0,0]])\n",
    "torch.where(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "c=torch.tensor([[False,True],[True,True]])\n",
    "c.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([ 3,  4, 10]),\n",
       "indices=tensor([2, 2, 2]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.tensor([[1,2,3],[1,2,4],[1,2,10]])\n",
    "outputs.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([1,2,3,4,5,6,7])\n",
    "gold = torch.tensor([1,2,3,3,3,3,3])\n",
    "non_padding = torch.tensor([False,True,True,False,False,False,True])\n",
    "pred.eq(gold).masked_select(non_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pred.size() == gold.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(pred.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[100, 100,   3],\n",
       "         [100, 100,   3],\n",
       "         [100, 100,   3]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "mask = torch.tensor([[True,True,False]]).unsqueeze(1)\n",
    "word_scores = torch.tensor([[[1,2,3],[1,2,3],[1,2,3]]])\n",
    "word_scores.masked_fill(mask, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,1]).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 10]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full([3,1],10)[:,-1].view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/sda/hanqinyu/DPC/RODS-main/models/BERT_interact/data/CSDS/dict/label2vid.json','r',encoding = 'utf-8') as js_object:\n",
    "    label2vid = json.load(js_object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 5],\n",
       "        [1, 2, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.tensor([[1,2,3],[1,2,4],[1,2,5]])\n",
    "id = torch.tensor([0,2,1])\n",
    "a[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6, 14],\n",
       "         [ 7, 17],\n",
       "         [ 8, 20]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "a = torch.tensor([[[1,2,3],[1,2,4],[1,2,5]]])\n",
    "b = torch.tensor([[1,1,1],[1,2,3]])\n",
    "F.linear(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.5000, 2.0000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "sequence = torch.tensor([[1,1,1],[1,2,3]])\n",
    "sequence.sum(dim=0) / (sequence != 0).sum(dim=0)\n",
    "# (sequence != 0).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "embedding = nn.Embedding(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.5265,  1.2232, -0.5563],\n",
       "        [-0.7228,  0.7736, -0.8997],\n",
       "        [-0.3493,  0.3765, -1.8596],\n",
       "        [-0.5221,  0.0956, -1.7240],\n",
       "        [-1.0532, -0.0440, -1.3779],\n",
       "        [-0.2831,  0.6198,  1.0566],\n",
       "        [-0.4747, -0.0486,  0.8864],\n",
       "        [-0.5696,  1.4773, -0.3870],\n",
       "        [ 0.3113,  0.7729, -0.1542]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 3,padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Dropout(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.6581,  1.5289, -0.0000],\n",
       "        [-0.9035,  0.9670, -1.1246],\n",
       "        [-0.4366,  0.4706, -2.3245],\n",
       "        [-0.6526,  0.1195, -2.1550],\n",
       "        [-1.3165, -0.0550, -1.7223],\n",
       "        [-0.3539,  0.0000,  1.3207],\n",
       "        [-0.5933, -0.0000,  1.1080],\n",
       "        [-0.0000,  1.8467, -0.0000],\n",
       "        [ 0.0000,  0.9662, -0.1927]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[100,   2, 100],\n",
       "         [100,   2, 100],\n",
       "         [100,   2, 100]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "word_scores = torch.tensor([[[1,2,3],[1,2,3],[1,2,3]]])\n",
    "mask = torch.tensor([[True,False,True]]).unsqueeze(1)\n",
    "word_scores.masked_fill(mask, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = torch.tensor([[1,2,3],[1,2,3],[1,2,3]])\n",
    "a.max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "a = enumerate([0,1,2,3,4,5,6,7,8])\n",
    "for idx, topic_id in a:\n",
    "    print(topic_id)\n",
    "    if topic_id == 2:\n",
    "        next(a)\n",
    "        next(a)\n",
    "        next(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_fin_role = []\n",
    "tgt_fin_token_idxs = [1,2,3,4,5,6]\n",
    "shift = True\n",
    "for idx in tgt_fin_token_idxs:\n",
    "    if shift:\n",
    "        tgt_fin_role.append(1)\n",
    "    else:\n",
    "        tgt_fin_role.append(2)\n",
    "    if idx in [3,5]:\n",
    "        shift = not shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_fin_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "a = torch.tensor([[1,2,3,4],[1,2,3,5],[1,2,3,6]])\n",
    "b = torch.tensor([[1,2,3,4],[1,2,3,5],[1,2,3,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4],\n",
       "         [1, 2, 3, 5],\n",
       "         [1, 2, 3, 6]],\n",
       "\n",
       "        [[1, 2, 3, 4],\n",
       "         [1, 2, 3, 5],\n",
       "         [1, 2, 3, 6]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a,b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "a = torch.tensor([[[1,2,3,4],[1,2,3,5],[1,2,3,6]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EE\n"
     ]
    }
   ],
   "source": [
    "if a is not None:\n",
    "    print('EE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10],\n",
       "         [11],\n",
       "         [12]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.6931, 1.0986, 1.3863],\n",
       "         [0.0000, 0.6931, 1.0986, 1.6094],\n",
       "         [0.0000, 0.6931, 1.0986, 1.7918]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7311, 0.8808, 0.9526, 0.9820],\n",
       "         [0.7311, 0.8808, 0.9526, 0.9933],\n",
       "         [0.7311, 0.8808, 0.9526, 0.9975]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[1,2,3,4],[1,2,3,5],[1,2,3,6]]])\n",
    "b = torch.tensor([[[1,2,3,4],[1,2,3,5],[1,2,3,6]]])\n",
    "\n",
    "torch.sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 3, 4, 5],\n",
       "         [2, 3, 4, 6],\n",
       "         [2, 3, 4, 7]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "a = torch.tensor([[[[1,2,3,4],[1,2,3,5],[1,2,3,6]],[[1,2,3,13],[1,2,3,14],[1,2,3,15]]]])\n",
    "b = torch.tensor([[[[2],[3],[4]]]])\n",
    "\n",
    "c = torch.ones(10)\n",
    "c[1:8] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., 10., 10., 10., 10., 10., 10., 10.,  1.,  1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.tensor([[[1,2,3,4],[1,2,3,5],[1,2,3,6]]]).float()\n",
    "torch.zeros(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([[[1,2,0,4],[1,2,3,0],[1,0,3,6]]]).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False,  True, False],\n",
       "         [False, False, False,  True],\n",
       "         [False,  True, False, False]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.,  0.,  1.,  2.],\n",
       "         [-1.,  0.,  1.,  3.],\n",
       "         [-1.,  0.,  1.,  4.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[2., 4., 6., 9.]])\n",
      "tensor([[ 3.,  6.,  9., 15.]])\n",
      "tensor([[[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [0.5000, 0.5000, 0.5000, 0.5556],\n",
      "         [0.3333, 0.3333, 0.3333, 0.4000]]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1,4)\n",
    "for i in range(3):\n",
    "    b += a[:,i,:]\n",
    "    a[:,i,:] = a[:,i,:]/b\n",
    "    print(b)\n",
    "# # a[:,1,:] = a[:,1,:]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4],\n",
       "         [ 1,  2,  3,  3]],\n",
       "\n",
       "        [[ 1,  2,  3,  5],\n",
       "         [ 1,  2,  3,  4]],\n",
       "\n",
       "        [[ 1,  2,  3,  6],\n",
       "         [ 1,  2,  3, 20]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4],[1,2,3,5],[1,2,3,6]])\n",
    "b = torch.tensor([[1,2,3,3],[1,2,3,4],[1,2,3,20]])\n",
    "torch.stack([a,b],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2,3,4],[1,2,3,5],[1,2,3,6]]])\n",
    "b = torch.tensor([2,3,4])\n",
    "c = torch.tensor([[[1,1,1,1],[1,1,1,1],[1,1,1,20]]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[True,True,False],[True,False,False]])\n",
    "a.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.0000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(a,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-10, 1.0000e-10]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1,2).fill_(1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4],\n",
       "         [ 1,  2,  3,  5],\n",
       "         [ 1,  2,  3,  6]],\n",
       "\n",
       "        [[ 1,  2,  3,  4],\n",
       "         [ 1,  2,  3, 20],\n",
       "         [ 0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "a = torch.tensor([[1,2,3,4],[1,2,3,5],[1,2,3,6]])\n",
    "b = torch.tensor([1,2,3,4])\n",
    "pad_sequence([a,b],batch_first = True,padding_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.arange(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.cat([torch.arange(1,12),torch.tensor([0])])\n",
    "b=torch.cat([torch.arange(1,8),torch.tensor([0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tgt=pad_sequence([a,b],batch_first=True,padding_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tgt[tag_tgt == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,  0],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0237e-18,  3.0677e-41, -3.2979e-18]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3708e-18,  3.0677e-41, -3.4086e-18]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty([1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[2,2,3,4],[1,2,3,5],[1,2,3,6]])\n",
    "a.eq(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 3, 4, 1, 2, 3, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[2,2,3,4],[1,2,3,5],[1,2,3,6]])\n",
    "b = torch.tensor([True,True,False])\n",
    "torch.masked_select(a,b.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shuffle_index = [i for i in range(7)]\n",
    "np.random.shuffle(shuffle_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 1, 5, 4, 0, 3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 4, 6, 2, 7, 1, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1,8)[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 6, 1, 0, 2, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(shuffle_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 5],\n",
       "        [1, 2, 3, 6],\n",
       "        [2, 2, 3, 4]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[2,2,3,4],[1,2,3,5],[1,2,3,6]])\n",
    "a[[1,2,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in range(0):\n",
    "    prine(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_i = [1,2,6,8,0]\n",
    "seggram = [0,3,4,2]\n",
    "len(set(seg_i) & set(seggram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_word2id(ref, pred):\n",
    "    # ref = re.sub(' ', '', ref)\n",
    "    # pred = re.sub(' ', '', pred)\n",
    "    # pred = re.sub('<q>', '', pred)\n",
    "    ref_id, pred_id = [], []\n",
    "    tmp_dict = {}\n",
    "    new_index = 0\n",
    "    words = list(ref)\n",
    "    for w in words:\n",
    "        if w not in tmp_dict.keys():\n",
    "            tmp_dict[w] = new_index\n",
    "            ref_id.append(str(new_index))\n",
    "            new_index += 1\n",
    "        else:\n",
    "            ref_id.append(str(tmp_dict[w]))\n",
    "    words = list(pred)\n",
    "    for w in words:\n",
    "        if w not in tmp_dict.keys():\n",
    "            tmp_dict[w] = new_index\n",
    "            pred_id.append(str(new_index))\n",
    "            new_index += 1\n",
    "        else:\n",
    "            pred_id.append(str(tmp_dict[w]))\n",
    "    return ' '.join(ref_id), ' '.join(pred_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1 2 3 4 5 6', '2 7 8 9 10 11 5 12 13 5 14 15 5 16 5 6')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "ref, pred = [ 0,  4,  8, 28,  2,  1], [ 4, 12, 14, 15, 16, 18,  2, 24, 27,  2, 29, 33,  2, 31,  2,  1]\n",
    "# change_word2id(ref, pred)\n",
    "change_word2id_split(ref, pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0 1 2 3 4 5', '1 6 7 8 9 10 4 11 12 4 13 14 4 15 4 5')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_word2id(ref, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_word2id_split(ref, pred, num):\n",
    "    if num:\n",
    "        pass\n",
    "    else:\n",
    "        ref = re.sub(' ', '', ref)\n",
    "        pred = re.sub(' ', '', pred)\n",
    "        ref = re.sub('<q>', '', ref)\n",
    "        pred = re.sub('<q>', '', pred)\n",
    "    ref_id, pred_id = [], []\n",
    "    tmp_dict = {'%': 0}\n",
    "    new_index = 1\n",
    "    words = list(ref)\n",
    "    for w in words:\n",
    "        if w not in tmp_dict.keys():\n",
    "            tmp_dict[w] = new_index\n",
    "            ref_id.append(str(new_index))\n",
    "            new_index += 1\n",
    "        else:\n",
    "            ref_id.append(str(tmp_dict[w]))\n",
    "        if w == '。':\n",
    "            ref_id.append(str(0))\n",
    "    words = list(pred)\n",
    "    for w in words:\n",
    "        if w not in tmp_dict.keys():\n",
    "            tmp_dict[w] = new_index\n",
    "            pred_id.append(str(new_index))\n",
    "            new_index += 1\n",
    "        else:\n",
    "            pred_id.append(str(tmp_dict[w]))\n",
    "        if w == '。':\n",
    "            pred_id.append(str(0))\n",
    "    return ' '.join(ref_id), ' '.join(pred_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "if a is not None:\n",
    "    print(\"!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@\n"
     ]
    }
   ],
   "source": [
    "value = \"不详。\"\n",
    "if value.strip().strip('。') in ['暂无','暂缺','无','不详']:\n",
    "    print('@@@@@@@@@@@@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WW\n"
     ]
    }
   ],
   "source": [
    "if \"\" in ['']:\n",
    "    print('WW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WW\n"
     ]
    }
   ],
   "source": [
    "a = ''\n",
    "if a.strip().strip('。') in ['']:\n",
    "    print('WW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9477,  0.4474, -0.7866],\n",
      "        [-0.8812,  0.9712, -0.0094],\n",
      "        [-2.6336, -0.4913,  0.8207]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "input = autograd.Variable(torch.randn(3,3), requires_grad=True)\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2793, 0.6100, 0.3129],\n",
      "        [0.2929, 0.7254, 0.4976],\n",
      "        [0.0670, 0.3796, 0.6944]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "sigmoid= nn.Sigmoid()\n",
    "print(sigmoid(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.FloatTensor([[[0, 1, 1], [1, 1, 1], [0, 0, 0]]])\n",
    "input = input.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3276, 0.4943, 1.1619],\n",
      "         [1.2278, 0.3211, 0.6979],\n",
      "         [0.0694, 0.4774, 1.1855]]], grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.BCELoss(reduction='none')\n",
    "print(loss(sigmoid(input), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False, False,  True],\n",
       "        [ True, False, False, False,  True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1,2,0,0,3],[1,0,0,0,3]]).bool()\n",
    "a&a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_cls = torch.tensor([[True,False]])\n",
    "np.where(mask_cls.cpu().data.numpy()[0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for j, label in enumerate(intent_label[0]):\n",
    "    if label == 1:\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '3', '3', '3']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '123333'\n",
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'others'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b1a5cd34dd1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcal_rouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcal_rouge_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcal_rouge_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/sda/hanqinyu/DPC/RODS-main/models/BERT_interact_former/logs/bert_both_ft__test_.1500.candidate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/home/sda/hanqinyu/DPC/RODS-main/models/BERT_interact_former/logs/bert_both_ft__test_.1500.gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'others'"
     ]
    }
   ],
   "source": [
    "from cal_rouge import cal_rouge_path\n",
    "cal_rouge_path('/home/sda/hanqinyu/DPC/RODS-main/models/BERT_interact_former/logs/bert_both_ft__test_.1500.candidate', '/home/sda/hanqinyu/DPC/RODS-main/models/BERT_interact_former/logs/bert_both_ft__test_.1500.gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a36d8463a67757e117c46e5ab6995ae383c8beeefc962ac5c4462ebc237a4882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
